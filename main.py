import torch
import pickle

import causalities
import data_generators as dg


if __name__ == "__main__":
    encoder_dict = {
        "flow_temporal_encoder": {
            "attention_layers": 3,
            "attention_heads": 3,
            "attention_dim": 6,
            "attention_feedforward_dim": 2,
            "dropout": 0.0,
            "masked_time_series": None,
        },
    }

    model_parameters = {
        "flow_series_embedding_dim": 2,
        "flow_input_encoder_layers": 2,
        "bagging_size": None,
        "input_encoding_normalization": True,
        "data_normalization": "none",
        "loss_normalization": "both",
        "positional_encoding": {
            "dropout": 0.1,
        },
        **encoder_dict,
        "copula_decoder": {
            # flow_input_dim and copula_input_dim are passed by the TACTIS module dynamically
            "min_u": 0.05,
            "max_u": 0.95,
            "dsf_marginal": {
                "mlp_layers": 2,
                "mlp_dim": 6,
                "flow_layers": 2,
                "flow_hid_dim": 8,
            },
        },
    }

    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    # num_samples = 2000
    # time_steps = 3
    # x = torch.randn(num_samples, 1, time_steps)
    # y = torch.randn(num_samples, 1, time_steps)
    # z = x + y
    # z0 = torch.randn(num_samples, 1, 1)
    # z = torch.cat([z0, z], dim=2)
    # z = z[:, :, :-1]
    #
    # input_tokens = torch.cat([x, y, z], dim=1)
    # print(input_tokens.size())

    # num_samples = 1000
    # data_generator = dg.Cut_V(4, 0.5, "cut_v")
    # data = []
    # for _ in range(num_samples):
    #     data.append(torch.transpose(data_generator.generate_data(10), 1, 0))
    # data = torch.stack(data).to(device)
    # causal_graph = data_generator.true_causal_graph()
    #
    # with open("simple_data.pkl", "wb") as f:
    #     pickle.dump(input_tokens, f)

    with open("simple_data.pkl", "rb") as f:
        input_tokens = pickle.load(f)
    input_tokens = input_tokens.to(device)

    # print("true causality graph:", causal_graph)

    # true_causality_graph = torch.stack(causalities.generate_causalities(input_tokens[:1000], "retrain", model_parameters))
    # print("causality graph generated by the grand true method:")
    # print(true_causality_graph)

    causality_graph = torch.stack(causalities.generate_causalities(input_tokens[:1000], "zero attention score", model_parameters))
    print("causality graph generated by the zero attention score method:")
    print(causality_graph)

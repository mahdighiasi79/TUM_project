import torch
import pickle
from sklearn.feature_selection import mutual_info_regression
from scipy.stats import entropy
import numpy as np

from causalities import Causalities
import data_generators as dg
from utils import Experiment


if __name__ == "__main__":
    encoder_dict = {
        "flow_temporal_encoder": {
            "attention_layers": 3,
            "attention_heads": 3,
            "attention_dim": 6,
            "attention_feedforward_dim": 5,
            "dropout": 0.1,
            "masked_time_series": None,
        },
    }

    model_parameters = {
        "flow_series_embedding_dim": 2,
        "flow_input_encoder_layers": 2,
        "bagging_size": None,
        "input_encoding_normalization": True,
        "data_normalization": "none",
        "loss_normalization": "both",
        "positional_encoding": {
            "dropout": 0.1,
        },
        **encoder_dict,
        "copula_decoder": {
            # flow_input_dim and copula_input_dim are passed by the TACTIS module dynamically
            "min_u": 0.05,
            "max_u": 0.95,
            "dsf_marginal": {
                "mlp_layers": 3,
                "mlp_dim": 6,
                "flow_layers": 3,
                "flow_hid_dim": 8,
            },
        },
    }

    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    # num_samples = 1000
    # time_steps = 2
    # x = torch.sin(torch.arange(num_samples * time_steps)).reshape(num_samples, 1, time_steps) * 10
    # x += torch.rand(num_samples, 1, time_steps)
    # y = torch.cos(torch.arange(num_samples * time_steps)).reshape(num_samples, 1, time_steps) * 10
    # y += torch.rand(num_samples, 1, time_steps)
    # z = x + y
    # z0 = torch.rand(num_samples, 1, 1) * 100
    # z = torch.cat([z0, z], dim=2)
    # z = z[:, :, :-1]
    # z_addition = torch.arange(num_samples * time_steps).reshape(num_samples, 1, time_steps)
    # z += (torch.randn(num_samples, 1, time_steps) * 10) + 10
    # input_tokens = torch.cat([x, y, z], dim=1).to(device)
    # print(input_tokens[:10])

    # num_samples = 1000
    # data_generator = dg.Cut_V(4, 0.5, "sigmoid")
    # data = []
    # for _ in range(num_samples):
    #     data.append(torch.transpose(data_generator.generate_data(10), 1, 0))
    # data = torch.stack(data).to(device)
    # causal_graph = data_generator.true_causal_graph()
    # print("true causal graph:\n", causal_graph)
    # input_tokens = torch.tensor(data).to(device)

    # with open("simple_data.pkl", "wb") as f:
    #     pickle.dump(input_tokens, f)

    # simple_experiment = Experiment(model_parameters, input_tokens, 0.001, 50, 30)
    # with open("exp_corr_xy_failed_methods.pkl", "wb") as f:
    #     pickle.dump(simple_experiment, f)

    with open("exp_corr_xy_failed_methods.pkl", "rb") as f:
        experiment = pickle.load(f)
    input_tokens = experiment.data

    input_tokens = input_tokens.to("cpu")
    num_samples, num_series, time_steps = input_tokens.shape
    x_numpy = input_tokens[:, 0, :].reshape(num_samples * time_steps).numpy()
    y_numpy = input_tokens[:, 1, :].reshape(num_samples * time_steps).numpy()
    z_numpy = input_tokens[:, 2, :].reshape(num_samples * time_steps).numpy()
    x_values, x_counts = np.unique(x_numpy, return_counts=True)
    y_values, y_counts = np.unique(y_numpy, return_counts=True)
    z_values, z_counts = np.unique(z_numpy, return_counts=True)
    x_probs = x_counts / x_counts.sum()
    y_probs = y_counts / y_counts.sum()
    z_probs = z_counts / z_counts.sum()
    h_x = entropy(x_probs)
    h_y = entropy(y_probs)
    h_z = entropy(z_probs)
    mi_xy = mutual_info_regression(x_numpy.reshape(-1, 1), y_numpy)
    mi_xz = mutual_info_regression(x_numpy.reshape(-1, 1), z_numpy)
    mi_yz = mutual_info_regression(y_numpy.reshape(-1, 1), z_numpy)
    print("H(X):", h_x)
    print("H(Y):", h_y)
    print("H(Z):", h_z)
    print("MI(X,Y):", mi_xy)
    print("MI(X,Z):", mi_xz)
    print("MI(Y,Z):", mi_yz)

    # input_tokens = dg.generate_static_nonlinear_data()
    input_tokens = input_tokens.to(device)
    # print(input_tokens[:10])
    # print(input_tokens.shape)

    causalities = Causalities(input_tokens, model_parameters)

    true_causality_graph = torch.stack(
        causalities.generate_causalities("retrain", 0.4))
    print("causality graph generated by the grand true method:")
    print(true_causality_graph)

    causality_graph = torch.stack(
        causalities.generate_causalities("zero attention score", 0.09))
    print("causality graph generated by the zero attention score method:")
    print(causality_graph)

    true_causality_graph = torch.stack(
        causalities.generate_causalities("constant series", 0.0))
    print("causality graph generated by the constant series method:")
    print(true_causality_graph)
